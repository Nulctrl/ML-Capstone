{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fde2d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8816daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table of environments- https://github.com/openai/gym/wiki/Table-of-environments\n",
    "ENV_NAME = 'MountainCar-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a85ba751",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env.reset()\n",
    "\n",
    "STATE_SIZE = len(env.observation_space.low)\n",
    "ACTION_SIZE = env.action_space.n\n",
    "\n",
    "done = False\n",
    "\n",
    "time.sleep(1)\n",
    "while not done:\n",
    "    action = random.randint(0,ACTION_SIZE-1)\n",
    "    new_state, reward, done, _ = env.step(action)    \n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635810b9",
   "metadata": {},
   "source": [
    "### Observation Space for MountainCar-v0\n",
    "    The observation is a `ndarray` with shape `(2,)` where the elements correspond to the following:\n",
    "    | Num | Observation                                                 | Min                | Max    | Unit |\n",
    "    |-----|-------------------------------------------------------------|--------------------|--------|------|\n",
    "    | 0   | position of the car along the x-axis                        | -Inf               | Inf    | position (m) |\n",
    "    | 1   | velocity of the car                                         | -Inf               | Inf  | position (m) |\n",
    "    ### Action Space\n",
    "    There are 3 discrete deterministic actions:\n",
    "    | Num | Observation                                                 | Value   | Unit |\n",
    "    |-----|-------------------------------------------------------------|---------|------|\n",
    "    | 0   | Accelerate to the left                                      | Inf    | position (m) |\n",
    "    | 1   | Don't accelerate                                            | Inf  | position (m) |\n",
    "    | 2   | Accelerate to the right                                     | Inf    | position (m) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e713d35",
   "metadata": {},
   "source": [
    "### Reward:\n",
    "    The goal is to reach the flag placed on top of the right hill as quickly as possible, as such the agent is penalised with a reward of -1 for each timestep it isn't at the goal and is not penalised (reward = 0) for when it reaches the goal.\n",
    "    ### Starting State\n",
    "    The position of the car is assigned a uniform random value in *[-0.6 , -0.4]*. The starting velocity of the car is always assigned to 0.\n",
    "    ### Episode Termination\n",
    "    The episode terminates if either of the following happens:\n",
    "    1. The position of the car is greater than or equal to 0.5 (the goal position on top of the right hill)\n",
    "    2. The length of the episode is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e8c360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bucket_sizes(ob_space_box,memory):\n",
    "    low,high = ob_space_box.low, ob_space_box.high\n",
    "    start = high[0] - low[0]\n",
    "    product = 1\n",
    "    for a,b in zip(low[1:],high[1:]):\n",
    "        product *= (b-a)/start\n",
    "    bucket_start = (memory/product)**(1/len(low))\n",
    "    mem_arr = [int(bucket_start)]\n",
    "    for a,b in zip(low[1:],high[1:]):\n",
    "        mem_arr.append(int((b-a)/start * bucket_start))\n",
    "    return mem_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18364162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the observation bounds of the state\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3024a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# actions we can take\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ac1a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_SIZE = len(env.observation_space.low)\n",
    "ACTION_SIZE = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11e7231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_SIZE = 16\n",
    "bucket_sizes = [BUCKET_SIZE] * STATE_SIZE\n",
    "\n",
    "def get_bucket_idx(num,s_idx):\n",
    "    \"\"\"\n",
    "    Util method to convert a continuous observation space value to bucket index\n",
    "    \"\"\"\n",
    "    low,hi = env.observation_space.low[s_idx],env.observation_space.high[s_idx]\n",
    "    offset = (num - low)/(hi - low)\n",
    "    return min(int(offset * bucket_sizes[s_idx]),bucket_sizes[s_idx]-1)\n",
    "\n",
    "def get_action_space(state):\n",
    "    \"\"\"\n",
    "    Util method to dynamically get state space. We do this to avoid any hard-coding \n",
    "    \"\"\"\n",
    "    s_idxs = [get_bucket_idx(num,i) for i,num in enumerate(state)]\n",
    "    action_space = q_table\n",
    "    for i in s_idxs:\n",
    "        action_space = action_space[i]\n",
    "    return action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11a7cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowest value gets bucket of index: 0\n",
      "-0.5 gets index: 6\n",
      "and so on\n",
      "highest value gets bucket of index: 15 which is maximum\n"
     ]
    }
   ],
   "source": [
    "# example of get_bucket_idx- 1st observed state lies in range [-1.2  0.6]\n",
    "print('lowest value gets bucket of index:',get_bucket_idx(-1.2,0))\n",
    "print('-0.5 gets index:',get_bucket_idx(-0.5,0))\n",
    "print('and so on')\n",
    "print('highest value gets bucket of index:',get_bucket_idx(0.6,0),'which is maximum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd889734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0a4c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.random.randn(*bucket_sizes,ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50fcc935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49009fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(old_state,new_state,action,reward):\n",
    "    \"\"\"\n",
    "    update happens after the action yields a reward.\n",
    "    We need updated state, old state, action and reward\n",
    "    as we go from old state to new state\n",
    "    \"\"\"\n",
    "    old_action_space,new_action_space = get_action_space(old_state), get_action_space(new_state)\n",
    "    temporal_diff = reward + discount_factor * max(new_action_space) - old_action_space[action]\n",
    "    old_action_space[action] += lr*temporal_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baff1cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendering this env to check progress.\n",
      "reward reward avg for 500th episode : -200.0\n",
      "rendering this env to check progress.\n",
      "reward reward avg for 1000th episode : -196.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4664/2477810382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_action_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mD:\\Aconda\\envs\\Lab1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[1;34m(a, axis, out)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \"\"\"\n\u001b[1;32m-> 1195\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Aconda\\envs\\Lab1\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "RENDER_EVERY = 1000\n",
    "\n",
    "for i in range(10000):\n",
    "    if i%1000 == 0:\n",
    "        print('rendering this env to check progress.')\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(get_action_space(state))\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        update_q_table(state,new_state,action,reward)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        if i % RENDER_EVERY == 0:\n",
    "            env.render()\n",
    "    \n",
    "    if i%500 == 0 and i>10:\n",
    "        print(f'reward reward avg for {i}th episode :',np.average(total_rewards[-10:]))\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ab9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
